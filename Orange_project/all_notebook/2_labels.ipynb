{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File /allah/freqtrade/json_dict/2023-09-10_16-54-32_MacdStrategyCombined.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/allah/freqtrade/json_dict/2_labels.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/2_labels.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/2_labels.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# df_short = pd.read_json('/allah/freqtrade/json_dict/2023-09-06_17-41-16_MacdStrategyShort.json')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/2_labels.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# df_long = pd.read_json('/allah/freqtrade/json_dict/2023-09-10_14-30-20_MacdStrategyLong.json')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/2_labels.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# df_long = pd.read_json('/allah/freqtrade/json_dict/2023-09-06_18-00-38_MacdStrategyLong.json')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/2_labels.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m df_combined \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_json(\u001b[39m'\u001b[39;49m\u001b[39m/allah/freqtrade/json_dict/2023-09-10_16-54-32_MacdStrategyCombined.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:760\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[39mif\u001b[39;00m convert_axes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m orient \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    758\u001b[0m     convert_axes \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 760\u001b[0m json_reader \u001b[39m=\u001b[39m JsonReader(\n\u001b[1;32m    761\u001b[0m     path_or_buf,\n\u001b[1;32m    762\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[1;32m    763\u001b[0m     typ\u001b[39m=\u001b[39;49mtyp,\n\u001b[1;32m    764\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    765\u001b[0m     convert_axes\u001b[39m=\u001b[39;49mconvert_axes,\n\u001b[1;32m    766\u001b[0m     convert_dates\u001b[39m=\u001b[39;49mconvert_dates,\n\u001b[1;32m    767\u001b[0m     keep_default_dates\u001b[39m=\u001b[39;49mkeep_default_dates,\n\u001b[1;32m    768\u001b[0m     precise_float\u001b[39m=\u001b[39;49mprecise_float,\n\u001b[1;32m    769\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[1;32m    770\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    771\u001b[0m     lines\u001b[39m=\u001b[39;49mlines,\n\u001b[1;32m    772\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m    773\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    774\u001b[0m     nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[1;32m    775\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    776\u001b[0m     encoding_errors\u001b[39m=\u001b[39;49mencoding_errors,\n\u001b[1;32m    777\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[1;32m    778\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m    779\u001b[0m )\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m chunksize:\n\u001b[1;32m    782\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:861\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    860\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mujson\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 861\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data_from_filepath(filepath_or_buffer)\n\u001b[1;32m    862\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:917\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    909\u001b[0m     filepath_or_buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n\u001b[1;32m    910\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    911\u001b[0m     \u001b[39misinstance\u001b[39m(filepath_or_buffer, \u001b[39mstr\u001b[39m)\n\u001b[1;32m    912\u001b[0m     \u001b[39mand\u001b[39;00m filepath_or_buffer\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    916\u001b[0m ):\n\u001b[0;32m--> 917\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00mfilepath_or_buffer\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    919\u001b[0m \u001b[39mreturn\u001b[39;00m filepath_or_buffer\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File /allah/freqtrade/json_dict/2023-09-10_16-54-32_MacdStrategyCombined.json does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_combined = pd.read_json('/allah/freqtrade/json_dict/2023-09-10_16-54-32_MacdStrategyCombined.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 30 Volume Version \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "lag_features = ['volume']  # List of features to create lag columns for\n",
    "lag_columns = []  # List to store the names of generated lag columns\n",
    "\n",
    "for feature in lag_features:\n",
    "    for i in range(1, 21):\n",
    "        df_combined[f'{feature}_{i}'] = df_combined[feature].shift(i)\n",
    "        # Append the generated lag column names to the lag_columns list\n",
    "        lag_columns.append(f'{feature}_{i}')\n",
    "\n",
    "# Now, lag_columns contains the names of all generated lag columns\n",
    "# df_long['macd_direction'] = np.where(df_long.macdhist > 0, 1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before oversampling: Counter({0: 16434, 1: 5197})\n",
      "Class distribution after oversampling: Counter({1: 16434, 0: 16434})\n",
      "X_train shape: (21631, 20)\n",
      "y_train shape: (21631,)\n",
      "Epoch 1/5000\n",
      "1028/1028 [==============================] - 2s 1ms/step - loss: 0.6803 - accuracy: 0.5591 - val_loss: 0.6877 - val_accuracy: 0.4832\n",
      "Epoch 2/5000\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.6758 - accuracy: 0.5713 - val_loss: 0.6926 - val_accuracy: 0.5050\n",
      "Epoch 3/5000\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.6748 - accuracy: 0.5700 - val_loss: 0.6917 - val_accuracy: 0.5146\n",
      "Epoch 4/5000\n",
      "1028/1028 [==============================] - 1s 1ms/step - loss: 0.6739 - accuracy: 0.5736 - val_loss: 0.6903 - val_accuracy: 0.5009\n",
      "Epoch 5/5000\n",
      " 698/1028 [===================>..........] - ETA: 0s - loss: 0.6735 - accuracy: 0.5723"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/allah/freqtrade/json_dict/2_labels.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f746f6b796f5f31227d/allah/freqtrade/json_dict/2_labels.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f746f6b796f5f31227d/allah/freqtrade/json_dict/2_labels.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m tensorboard \u001b[39m=\u001b[39m TensorBoard(log_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./logs\u001b[39m\u001b[39m'\u001b[39m, histogram_freq\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, write_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, write_images\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f746f6b796f5f31227d/allah/freqtrade/json_dict/2_labels.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_resampled, y_train_resampled, epochs\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test_scaled, y_test), callbacks\u001b[39m=\u001b[39;49m[])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f746f6b796f5f31227d/allah/freqtrade/json_dict/2_labels.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Step 6: Evaluate the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f746f6b796f5f31227d/allah/freqtrade/json_dict/2_labels.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m test_loss, test_accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test_scaled, y_test)\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:147\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m--> 147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mconcrete_function\u001b[39m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:359\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m current_func_context \u001b[39m=\u001b[39m function_context\u001b[39m.\u001b[39mmake_function_context()\n\u001b[1;32m    355\u001b[0m \u001b[39m# cache_key_deletion_observer is useless here. It's based on all captures.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m# A new cache key will be built later when saving ConcreteFunction because\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[39m# only active captures should be saved.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m lookup_func_type, lookup_func_context \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_function_spec\u001b[39m.\u001b[39;49mmake_canonicalized_monomorphic_type(\n\u001b[1;32m    360\u001b[0m         args, kwargs, captures))\n\u001b[1;32m    361\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mlookup(current_func_context,\n\u001b[1;32m    362\u001b[0m                                                 lookup_func_type)\n\u001b[1;32m    363\u001b[0m \u001b[39mif\u001b[39;00m concrete_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py:324\u001b[0m, in \u001b[0;36mFunctionSpec.make_canonicalized_monomorphic_type\u001b[0;34m(self, args, kwargs, captures)\u001b[0m\n\u001b[1;32m    316\u001b[0m   captures \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m    318\u001b[0m kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    319\u001b[0m     function_type_lib\u001b[39m.\u001b[39msanitize_arg_name(name): value\n\u001b[1;32m    320\u001b[0m     \u001b[39mfor\u001b[39;00m name, value \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    321\u001b[0m }\n\u001b[1;32m    323\u001b[0m _, function_type, type_context \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 324\u001b[0m     function_type_lib\u001b[39m.\u001b[39;49mcanonicalize_to_monomorphic(\n\u001b[1;32m    325\u001b[0m         args, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault_values, captures, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m )\n\u001b[1;32m    329\u001b[0m \u001b[39mreturn\u001b[39;00m function_type, type_context\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_type.py:431\u001b[0m, in \u001b[0;36mcanonicalize_to_monomorphic\u001b[0;34m(args, kwargs, default_values, captures, polymorphic_type)\u001b[0m\n\u001b[1;32m    429\u001b[0m parameters \u001b[39m=\u001b[39m []\n\u001b[1;32m    430\u001b[0m type_context \u001b[39m=\u001b[39m trace_type\u001b[39m.\u001b[39mInternalTracingContext()\n\u001b[0;32m--> 431\u001b[0m has_var_positional \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(p\u001b[39m.\u001b[39mkind \u001b[39mis\u001b[39;00m Parameter\u001b[39m.\u001b[39mVAR_POSITIONAL\n\u001b[1;32m    432\u001b[0m                          \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m polymorphic_type\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m    434\u001b[0m \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m poly_bound_arguments\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    435\u001b[0m   poly_parameter \u001b[39m=\u001b[39m polymorphic_type\u001b[39m.\u001b[39mparameters[name]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "# Assuming you have already loaded your data\n",
    "df_long = df_combined[(df_combined.label == 'true_long') | (df_combined.label == 'false_long')]\n",
    "# Step 1: Select features and target variable\n",
    "features = lag_columns\n",
    "target = 'label'\n",
    "\n",
    "# Encode the labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_df = df_long[df_long.trade == 1.0]\n",
    "train_df['label'] = label_encoder.fit_transform(train_df['label'])\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df[features], train_df[target], test_size=0.2,shuffle=False, random_state=42\n",
    ")\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     train_df[features], train_df[target], test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# Step 3: Scale the features (MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled, X_test_scaled = scaler.fit_transform(X_train), scaler.transform(X_test)\n",
    "\n",
    "# Count the number of samples in each class\n",
    "print(\"Class distribution before oversampling:\", Counter(y_train))\n",
    "\n",
    "# Apply SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Count the number of samples in each class after oversampling\n",
    "print(\"Class distribution after oversampling:\", Counter(y_train_resampled))\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "# Step 4: Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(60, input_dim=len(features), activation='relu'),\n",
    "    Dense(30, activation='relu'),\n",
    "    Dense(3, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification, so use 'sigmoid' activation\n",
    "])\n",
    "\n",
    "# Specify a higher learning rate for the Adam optimizer\n",
    "learning_rate = 0.001  # You can adjust this value\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model with the specified optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train the model with TensorBoard\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True, write_images=True)\n",
    "model.fit(X_train_resampled, y_train_resampled, epochs=5000, batch_size=32, validation_data=(X_test_scaled, y_test), callbacks=[])\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Inverse transform the encoded labels to get the original labels\n",
    "# original_labels = label_encoder.inverse_transform(model.predict(X_test_scaled) > 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming you have already loaded your data\n",
    "# df_long = df_combined[(df_combined.label == 'true_long') | (df_combined.label == 'false_long')]\n",
    "\n",
    "# Step 1: Select features and target variable\n",
    "features = lag_columns\n",
    "target = 'label'\n",
    "\n",
    "# Encode the labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# train_df = df_long[df_long.trade == 1.0]\n",
    "# Assuming you have already loaded your data and encoded labels\n",
    "# train_df['label'] = label_encoder.fit_transform(train_df['label'])\n",
    "\n",
    "# Sample data (replace this with your actual data)\n",
    "X_train, y_train, X_test, y_test = np.random.rand(100, len(features)), np.random.randint(0, 2, 100), np.random.rand(20, len(features)), np.random.randint(0, 2, 20)\n",
    "\n",
    "# Step 2: Scale the features (MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled, X_test_scaled = scaler.fit_transform(X_train), scaler.transform(X_test)\n",
    "\n",
    "# Count the number of samples in each class\n",
    "print(\"Class distribution before oversampling:\", Counter(y_train))\n",
    "\n",
    "# Apply SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Count the number of samples in each class after oversampling\n",
    "print(\"Class distribution after oversampling:\", Counter(y_train_resampled))\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# Step 3: Define a simple neural network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def initialize_params(self, num_features):\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "    def compute_loss(self, y, y_pred):\n",
    "        m = len(y)\n",
    "        return -1/m * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "    def fit(self, X, y, learning_rate, num_epochs):\n",
    "        m, num_features = X.shape\n",
    "        self.initialize_params(num_features)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            y_pred = self.forward(X)\n",
    "\n",
    "            dw = (1/m) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1/m) * np.sum(y_pred - y)\n",
    "\n",
    "            self.weights -= learning_rate * dw\n",
    "            self.bias -= learning_rate * db\n",
    "\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.forward(X)\n",
    "        return (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Step 4: Train the simple neural network\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5000\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.fit(X_train_resampled, y_train_resampled, learning_rate, num_epochs)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "accuracy = np.mean(y_pred_test == y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
