{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining CSV files:   0%|          | 0/479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining CSV files: 100%|██████████| 479/479 [00:54<00:00,  8.76it/s] \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, base_url, zip_download_dir, csv_extract_dir):\n",
    "        self.base_url = base_url\n",
    "        self.zip_download_dir = zip_download_dir\n",
    "        self.csv_extract_dir = csv_extract_dir\n",
    "\n",
    "    def _download_zip_file(self, link, filename):\n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(filename):\n",
    "            print(f\"File {filename} already exists. Skipping download.\")\n",
    "            return True\n",
    "\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - Unable to download the ZIP file for {filename}\")\n",
    "            return False\n",
    "\n",
    "    def _extract_zip_file(self, zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            for file_info in zip_ref.infolist():\n",
    "                # Construct the full path for the extracted file\n",
    "                extracted_file_path = os.path.join(self.csv_extract_dir, file_info.filename)\n",
    "\n",
    "                # Check if the file already exists, and skip extraction if it does\n",
    "                if os.path.exists(extracted_file_path):\n",
    "                    print(f\"File {extracted_file_path} already exists. Skipping extraction.\")\n",
    "                    continue\n",
    "\n",
    "                # Extract the file\n",
    "                zip_ref.extract(file_info, path=self.csv_extract_dir)\n",
    "\n",
    "    def download_and_extract_data(self, start_date, end_date):\n",
    "        os.makedirs(self.zip_download_dir, exist_ok=True)\n",
    "        os.makedirs(self.csv_extract_dir, exist_ok=True)\n",
    "        os.makedirs(self.feather_stored_dir, exist_ok=True)\n",
    "        download_bar = tqdm(total=(end_date - start_date).days + 1, desc=\"Downloading and Extracting\")\n",
    "        \n",
    "        for current_date in (start_date + datetime.timedelta(n) for n in range((end_date - start_date).days + 1)):\n",
    "            date_str = current_date.strftime('%Y-%m-%d')\n",
    "            link = f'{self.base_url}ETHUSDT-aggTrades-{date_str}.zip'\n",
    "            filename = os.path.join(self.zip_download_dir, f'ETHUSDT-aggTrades-{date_str}.zip')\n",
    "\n",
    "            if self._download_zip_file(link, filename):\n",
    "                self._extract_zip_file(filename)\n",
    "            download_bar.update(1)\n",
    "        download_bar.close()\n",
    "\n",
    "    def process_csv_files(self):\n",
    "        csv_files = [file for file in os.listdir(self.csv_extract_dir) if file.endswith('.csv')]\n",
    "        csv_files.sort()\n",
    "        combined_df = None\n",
    "        progress_bar = tqdm(total=len(csv_files), desc=\"Combining CSV files\")\n",
    "        \n",
    "        for csv_file in csv_files:\n",
    "            file_path = os.path.join(self.csv_extract_dir, csv_file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            if combined_df is None:\n",
    "                combined_df = df\n",
    "            else:\n",
    "                combined_df = pd.concat([combined_df, df])\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        progress_bar.close()\n",
    "        return combined_df\n",
    "\n",
    "    def _convert_to_seconds(self, df):\n",
    "        df['utc_time'] = pd.to_datetime(df['transact_time'], unit='ms')\n",
    "        df.set_index('utc_time', inplace=True)\n",
    "        \n",
    "        resampled_df = df.resample('S').agg({\n",
    "            'agg_trade_id': 'first',\n",
    "            'price': 'ohlc',\n",
    "            'quantity': 'sum',\n",
    "            'first_trade_id': 'first',\n",
    "            'last_trade_id': 'last',\n",
    "            'is_buyer_maker': 'last',\n",
    "            # 'transact_time': 'last',\n",
    "        })\n",
    "        resampled_df.reset_index(inplace=True)\n",
    "        resampled_df.columns = resampled_df.columns.droplevel()\n",
    "        resampled_df.fillna(method='ffill', inplace=True)\n",
    "        resampled_df.fillna(method='bfill', inplace=True)\n",
    "        \n",
    "        return resampled_df\n",
    "\n",
    "    def run_data_processing_workflow(self, start_date, end_date):\n",
    "        self.download_and_extract_data(start_date, end_date)\n",
    "        df = self.process_csv_files()\n",
    "        df = self._convert_to_seconds(df)\n",
    "        # define start and end date of df\n",
    "        # df = df[(df['utc_time'] >= start_date) & (df['utc_time'] <= end_date)]\n",
    "        return df\n",
    "\n",
    "    def combine_select_csv_to_df(self, start_date, end_date):\n",
    "        csv_files = [file for file in os.listdir(self.csv_extract_dir) if file.endswith('.csv')]\n",
    "        csv_files.sort()\n",
    "        combined_df = None\n",
    "        progress_bar = tqdm(total=len(csv_files), desc=\"Combining CSV files\")\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            # Extract the date part from the file name and convert it to a date\n",
    "            date_str = '-'.join(csv_file.split('-')[-3:]).replace('.csv', '')\n",
    "            file_date = datetime.datetime.strptime(date_str, '%Y-%m-%d').date()\n",
    "\n",
    "            # Check if the file date is within the specified date range\n",
    "            if start_date <= file_date <= end_date:\n",
    "                file_path = os.path.join(self.csv_extract_dir, csv_file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                df = self._convert_to_seconds(df)  # Apply _convert_to_seconds\n",
    "                combined_df = pd.concat([combined_df, df]) if combined_df is not None else df\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "        return combined_df\n",
    "\n",
    "\n",
    "\n",
    "    def delete_all_data(self):\n",
    "        os.system(f'rm -rf {self.zip_download_dir}')\n",
    "        os.system(f'rm -rf {self.csv_extract_dir}')\n",
    "\n",
    "# Example usage:\n",
    "base_url = 'https://data.binance.vision/data/futures/um/daily/aggTrades/ETHUSDT/'\n",
    "zip_download_dir = '/allah/freqtrade/json_dict/binance_aggTrades'\n",
    "csv_extract_dir = '/allah/freqtrade/json_dict/decompressed_csv'\n",
    "\n",
    "data_processor = DataProcessor(base_url, zip_download_dir, csv_extract_dir)\n",
    "start_date = datetime.date(2023, 9, 15)\n",
    "end_date = datetime.date(2023, 10, 1)\n",
    "\n",
    "# df = data_processor.run_data_processing_workflow(start_date, end_date)\n",
    "# df = data_processor.download_and_extract_data(datetime.date(2023, 7, 1), datetime.date(2023, 10, 20))\n",
    "df = data_processor.combine_select_csv_to_df(datetime.date(2023, 4, 13), datetime.date(2023, 10, 20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataProcessor.__init__() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/allah/freqtrade/json_dict/seconds_data_generator.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/seconds_data_generator.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=159'>160</a>\u001b[0m csv_extract_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/allah/freqtrade/json_dict/decompressed_csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/seconds_data_generator.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=160'>161</a>\u001b[0m feather_stored_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/allah/freqtrade/json_dict/feather_data\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/seconds_data_generator.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=162'>163</a>\u001b[0m data_processor \u001b[39m=\u001b[39m DataProcessor(base_url, zip_download_dir, csv_extract_dir, feather_stored_dir)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/seconds_data_generator.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=163'>164</a>\u001b[0m start_date \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdate(\u001b[39m2023\u001b[39m, \u001b[39m9\u001b[39m, \u001b[39m15\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/seconds_data_generator.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=164'>165</a>\u001b[0m end_date \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdate(\u001b[39m2023\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: DataProcessor.__init__() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import feather\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, base_url, zip_download_dir, csv_extract_dir, feather_stored_dir):\n",
    "        self.base_url = base_url\n",
    "        self.zip_download_dir = zip_download_dir\n",
    "        self.csv_extract_dir = csv_extract_dir\n",
    "        self.feather_stored_dir = feather_stored_dir\n",
    "\n",
    "    def download_file(self, url, destination):\n",
    "        if os.path.exists(destination):\n",
    "            print(f\"File {destination} already exists. Skipping download.\")\n",
    "            return\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(destination, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Downloaded {destination}\")\n",
    "\n",
    "    def download_and_extract_data(self, start_date, end_date):\n",
    "        os.makedirs(self.zip_download_dir, exist_ok=True)\n",
    "        os.makedirs(self.csv_extract_dir, exist_ok=True)\n",
    "        os.makedirs(self.feather_stored_dir, exist_ok=True)\n",
    "        download_bar = tqdm(total=(end_date - start_date).days + 1, desc=\"Downloading and Extracting\")\n",
    "\n",
    "        for current_date in (start_date + datetime.timedelta(n) for n in range((end_date - start_date).days + 1)):\n",
    "            date_str = current_date.strftime('%Y-%m-%d')\n",
    "            zip_url = f'{self.base_url}ETHUSDT-aggTrades-{date_str}.zip'\n",
    "            zip_filename = os.path.join(self.zip_download_dir, f'ETHUSDT-aggTrades-{date_str}.zip')\n",
    "\n",
    "            self.download_file(zip_url, zip_filename)\n",
    "\n",
    "            with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "                for file_info in zip_ref.infolist():\n",
    "                    extracted_file_path = os.path.join(self.csv_extract_dir, file_info.filename)\n",
    "\n",
    "                    if not os.path.exists(extracted_file_path):\n",
    "                        zip_ref.extract(file_info, path=self.csv_extract_dir)\n",
    "                        print(f\"Extracted {extracted_file_path}\")\n",
    "                    else:\n",
    "                        print(f\"File {extracted_file_path} already exists. Skipping extraction.\")\n",
    "\n",
    "            download_bar.update(1)\n",
    "        download_bar.close()\n",
    "\n",
    "    def transfer_to_feather(self, csv_file_path, feather_file_path):\n",
    "        if not os.path.exists(feather_file_path):\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            df = self._convert_to_seconds(df)\n",
    "            df.to_feather(feather_file_path)\n",
    "            print(f\"Transferred {csv_file_path} to Feather format.\")\n",
    "        else:\n",
    "            print(f\"Feather file for {csv_file_path} already exists. Skipping transfer.\")\n",
    "\n",
    "    def transfer_extracted_csv_to_feather(self):\n",
    "        os.makedirs(self.feather_stored_dir, exist_ok=True)\n",
    "        csv_files = [file for file in os.listdir(self.csv_extract_dir) if file.endswith('.csv')]\n",
    "        csv_files.sort()\n",
    "        transfer_bar = tqdm(total=len(csv_files), desc=\"Transferring CSV to Feather\")\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            csv_file_path = os.path.join(self.csv_extract_dir, csv_file)\n",
    "            feather_file_path = os.path.join(self.feather_stored_dir, f\"{csv_file.replace('.csv', '.feather')}\")\n",
    "            self.transfer_to_feather(csv_file_path, feather_file_path)\n",
    "            transfer_bar.update(1)\n",
    "        transfer_bar.close()\n",
    "\n",
    "    def process_csv_files(self):\n",
    "        csv_files = [file for file in os.listdir(self.csv_extract_dir) if file.endswith('.csv')]\n",
    "        csv_files.sort()\n",
    "        combined_df = None\n",
    "        progress_bar = tqdm(total=len(csv_files), desc=\"Combining CSV files\")\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            file_path = os.path.join(self.csv_extract_dir, csv_file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            if combined_df is None:\n",
    "                combined_df = df\n",
    "            else:\n",
    "                combined_df = pd.concat([combined_df, df])\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "        return combined_df\n",
    "\n",
    "    def _convert_to_seconds(self, df):\n",
    "        df['utc_time'] = pd.to_datetime(df['transact_time'], unit='ms')\n",
    "        df.set_index('utc_time', inplace=True)\n",
    "\n",
    "        resampled_df = df.resample('S').agg({\n",
    "            'agg_trade_id': 'first',\n",
    "            'price': 'ohlc',\n",
    "            'quantity': 'sum',\n",
    "            'first_trade_id': 'first',\n",
    "            'last_trade_id': 'last',\n",
    "            'is_buyer_maker': 'last',\n",
    "        })\n",
    "        resampled_df.reset_index(inplace=True)\n",
    "        resampled_df.columns = resampled_df.columns.droplevel()\n",
    "        resampled_df.fillna(method='ffill', inplace=True)\n",
    "        resampled_df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "        return resampled_df\n",
    "\n",
    "    def combine_to_df(self, data_dir, start_date, end_date):\n",
    "        data_files = [file for file in os.listdir(data_dir) if file.endswith('.feather')]\n",
    "        data_files.sort()\n",
    "        combined_df = None\n",
    "        progress_bar = tqdm(total=len(data_files), desc=\"Combining Feather Files\")\n",
    "\n",
    "        for data_file in data_files:\n",
    "            date_str = '-'.join(data_file.split('-')[-3:]).replace('.feather', '')\n",
    "            file_date = datetime.datetime.strptime(date_str, '%Y-%m-%d').date()\n",
    "\n",
    "            if start_date <= file_date <= end_date:\n",
    "                data_path = os.path.join(data_dir, data_file)\n",
    "                df = pd.read_feather(data_path)\n",
    "                df = self._convert_to_seconds(df)\n",
    "                combined_df = pd.concat([combined_df, df]) if combined_df is not None else df\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "        return combined_df\n",
    "\n",
    "    def delete_all_data(self):\n",
    "        os.system(f'rm -rf {self.zip_download_dir}')\n",
    "        os.system(f'rm -rf {self.csv_extract_dir}')\n",
    "        os.system(f'rm -rf {self.feather_stored_dir}')\n",
    "\n",
    "# Example usage:\n",
    "base_url = 'https://data.binance.vision/data/futures/um/daily/aggTrades/ETHUSDT/'\n",
    "zip_download_dir = '/allah/freqtrade/json_dict/aggTrades/binance_aggTrades'\n",
    "csv_extract_dir = '/allah/freqtrade/json_dict/aggTrades/decompressed_csv'\n",
    "feather_stored_dir = '/allah/freqtrade/json_dict/aggTrades/feather_data'\n",
    "\n",
    "data_processor = DataProcessor(base_url, zip_download_dir, csv_extract_dir, feather_stored_dir)\n",
    "start_date = datetime.date(2023, 9, 15)\n",
    "end_date = datetime.date(2023, 10, 1)\n",
    "\n",
    "# Download and extract data\n",
    "data_processor.download_and_extract_data(start_date, end_date)\n",
    "\n",
    "# Transfer the extracted CSV files to Feather format\n",
    "data_processor.transfer_extracted_csv_to_feather()\n",
    "\n",
    "# Combine selected Feather files to a DataFrame\n",
    "combined_df = data_processor.combine_to_df(feather_stored_dir, datetime.date(2023, 4, 13), datetime.date(2023, 10, 20))\n",
    "\n",
    "# Perform operations with combined_df as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import feather\n",
    "\n",
    "def format_and_save_dataframe(input_df, output_path):\n",
    "    # Create a copy of the input DataFrame\n",
    "    df_formatted = input_df.copy()\n",
    "\n",
    "    # Define the end time for your time series\n",
    "    end_time = datetime.strptime('2023-10-16 23:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Create a new index based on a range with 1-minute frequency\n",
    "    new_index = pd.date_range(end=end_time, periods=len(df_formatted), freq='1T')\n",
    "\n",
    "    # Assign the new index to the DataFrame and rename columns\n",
    "    df_formatted.index = new_index\n",
    "    df_formatted = df_formatted.rename(columns={'': 'real 1s'})\n",
    "\n",
    "    # Reset the index to make the 'date' column a regular column\n",
    "    df_formatted = df_formatted.reset_index()\n",
    "\n",
    "    # Rename and format columns to match the target format\n",
    "    df_formatted['date'] = df_formatted['index']\n",
    "    df_formatted['volume'] = df_formatted['quantity']\n",
    "    # df_formatted['date'] = pd.to_datetime(df_formatted['date']).dt.strftime('%Y-%m-%d %H:%M:%S') + '+00:00'\n",
    "    df_formatted['date'] = pd.to_datetime(df_formatted['date'])\n",
    "\n",
    "    df_formatted = df_formatted[['date', 'open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "    # Save the formatted DataFrame to a Feather file\n",
    "    # feather.write_dataframe(df_formatted, output_path)\n",
    "    df_formatted.to_feather(\n",
    "            output_path, compression_level=9, compression='lz4')\n",
    "\n",
    "# Usage\n",
    "input_df = df.copy()  # Replace with your actual DataFrame\n",
    "output_path = '/allah/freqtrade/user_data/data/binance/futures/BTC_USDT_USDT-1m-futures.feather'\n",
    "format_and_save_dataframe(input_df, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
