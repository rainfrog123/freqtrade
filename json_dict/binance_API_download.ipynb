{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/283 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 283/283 [06:50<00:00,  1.45s/it]\n",
      "Extracting: 100%|██████████| 283/283 [01:46<00:00,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ZIP files downloaded and CSV files extracted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "import zipfile\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "\n",
    "# Define the base URL for downloading the ZIP files\n",
    "base_url = 'https://data.binance.vision/data/futures/um/daily/aggTrades/ETHUSDT/'\n",
    "\n",
    "# Set the download directory for ZIP files\n",
    "zip_download_dir = '/allah/freqtrade/json_dict/binance_aggTrades'\n",
    "# Set the directory for decompressed CSV files\n",
    "csv_extract_dir = '/allah/freqtrade/json_dict/decompressed_csv'\n",
    "\n",
    "# Create the download and extract directories if they don't exist\n",
    "os.makedirs(zip_download_dir, exist_ok=True)\n",
    "os.makedirs(csv_extract_dir, exist_ok=True)\n",
    "\n",
    "# Define the start and end dates for the data\n",
    "start_date = datetime.date(2023, 1, 1)\n",
    "end_date = datetime.date(2023, 10, 10)\n",
    "\n",
    "# Create a progress bar for downloading\n",
    "download_bar = tqdm(total=(end_date - start_date).days + 1, desc=\"Downloading\")\n",
    "\n",
    "# Iterate through dates and download files\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    # Generate the download link for the current date\n",
    "    date_str = current_date.strftime('%Y-%m-%d')\n",
    "    link = f'{base_url}ETHUSDT-aggTrades-{date_str}.zip'\n",
    "    \n",
    "    # Download the file\n",
    "    response = requests.get(link)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Get the filename from the URL\n",
    "        filename = os.path.join(zip_download_dir, f'ETHUSDT-aggTrades-{date_str}.zip')\n",
    "\n",
    "        # Save the downloaded file\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        download_bar.update(1)  # Update the progress bar\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - Unable to download the ZIP file for {date_str}.\")\n",
    "\n",
    "    # Move to the next date\n",
    "    current_date += datetime.timedelta(days=1)\n",
    "\n",
    "download_bar.close()  # Close the progress bar\n",
    "\n",
    "# Create a progress bar for extraction\n",
    "extract_bar = tqdm(total=len(os.listdir(zip_download_dir)), desc=\"Extracting\")\n",
    "\n",
    "# Extract all downloaded ZIP files\n",
    "for filename in os.listdir(zip_download_dir):\n",
    "    if filename.endswith('.zip'):\n",
    "        with zipfile.ZipFile(os.path.join(zip_download_dir, filename), 'r') as zip_ref:\n",
    "            zip_ref.extractall(csv_extract_dir)\n",
    "        extract_bar.update(1)  # Update the progress bar\n",
    "\n",
    "extract_bar.close()  # Close the progress bar\n",
    "\n",
    "print(\"All ZIP files downloaded and CSV files extracted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining CSV files:   2%|▏         | 5/283 [00:00<00:05, 47.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining CSV files: 100%|██████████| 283/283 [01:25<00:00,  3.32it/s]\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: Path /allah/freqtrade/json_dict/aggTrades/batch_data/batch_0.parquet points to a directory, but only file paths are supported. To construct a nested or union dataset pass a list of dataset objects instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/dask/backends.py:136\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/dask/dataframe/io/parquet/core.py:543\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, use_nullable_dtypes, dtype_backend, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, blocksize, aggregate_files, parquet_file_extension, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m     blocksize \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m read_metadata_result \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mread_metadata(\n\u001b[1;32m    544\u001b[0m     fs,\n\u001b[1;32m    545\u001b[0m     paths,\n\u001b[1;32m    546\u001b[0m     categories\u001b[39m=\u001b[39;49mcategories,\n\u001b[1;32m    547\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    548\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    549\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[1;32m    550\u001b[0m     gather_statistics\u001b[39m=\u001b[39;49mcalculate_divisions,\n\u001b[1;32m    551\u001b[0m     filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[1;32m    552\u001b[0m     split_row_groups\u001b[39m=\u001b[39;49msplit_row_groups,\n\u001b[1;32m    553\u001b[0m     blocksize\u001b[39m=\u001b[39;49mblocksize,\n\u001b[1;32m    554\u001b[0m     aggregate_files\u001b[39m=\u001b[39;49maggregate_files,\n\u001b[1;32m    555\u001b[0m     ignore_metadata_file\u001b[39m=\u001b[39;49mignore_metadata_file,\n\u001b[1;32m    556\u001b[0m     metadata_task_size\u001b[39m=\u001b[39;49mmetadata_task_size,\n\u001b[1;32m    557\u001b[0m     parquet_file_extension\u001b[39m=\u001b[39;49mparquet_file_extension,\n\u001b[1;32m    558\u001b[0m     dataset\u001b[39m=\u001b[39;49mdataset_options,\n\u001b[1;32m    559\u001b[0m     read\u001b[39m=\u001b[39;49mread_options,\n\u001b[1;32m    560\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_options,\n\u001b[1;32m    561\u001b[0m )\n\u001b[1;32m    563\u001b[0m \u001b[39m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39m# compatibility with a user-defined engine.\u001b[39;00m\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/dask/dataframe/io/parquet/arrow.py:532\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[0;34m(cls, fs, paths, categories, index, use_nullable_dtypes, dtype_backend, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39m# Stage 1: Collect general dataset information\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m dataset_info \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_collect_dataset_info(\n\u001b[1;32m    533\u001b[0m     paths,\n\u001b[1;32m    534\u001b[0m     fs,\n\u001b[1;32m    535\u001b[0m     categories,\n\u001b[1;32m    536\u001b[0m     index,\n\u001b[1;32m    537\u001b[0m     gather_statistics,\n\u001b[1;32m    538\u001b[0m     filters,\n\u001b[1;32m    539\u001b[0m     split_row_groups,\n\u001b[1;32m    540\u001b[0m     blocksize,\n\u001b[1;32m    541\u001b[0m     aggregate_files,\n\u001b[1;32m    542\u001b[0m     ignore_metadata_file,\n\u001b[1;32m    543\u001b[0m     metadata_task_size,\n\u001b[1;32m    544\u001b[0m     parquet_file_extension,\n\u001b[1;32m    545\u001b[0m     kwargs,\n\u001b[1;32m    546\u001b[0m )\n\u001b[1;32m    548\u001b[0m \u001b[39m# Stage 2: Generate output `meta`\u001b[39;00m\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/dask/dataframe/io/parquet/arrow.py:1047\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[0;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[39mif\u001b[39;00m ds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1047\u001b[0m     ds \u001b[39m=\u001b[39m pa_ds\u001b[39m.\u001b[39;49mdataset(\n\u001b[1;32m   1048\u001b[0m         paths,\n\u001b[1;32m   1049\u001b[0m         filesystem\u001b[39m=\u001b[39;49m_wrapped_fs(fs),\n\u001b[1;32m   1050\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_processed_dataset_kwargs,\n\u001b[1;32m   1051\u001b[0m     )\n\u001b[1;32m   1053\u001b[0m \u001b[39m# Get file_frag sample and extract physical_schema\u001b[39;00m\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/pyarrow/dataset.py:776\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(_is_path_like(elem) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n\u001b[0;32m--> 776\u001b[0m     \u001b[39mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    777\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(elem, Dataset) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/pyarrow/dataset.py:454\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(source, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 454\u001b[0m     fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[1;32m    455\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/pyarrow/dataset.py:375\u001b[0m, in \u001b[0;36m_ensure_multiple_sources\u001b[0;34m(paths, filesystem)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[39melif\u001b[39;00m file_type \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mDirectory:\n\u001b[0;32m--> 375\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIsADirectoryError\u001b[39;00m(\n\u001b[1;32m    376\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mPath \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m points to a directory, but only file paths are \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    377\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msupported. To construct a nested or union dataset pass \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    378\u001b[0m         \u001b[39m'\u001b[39m\u001b[39ma list of dataset objects instead.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(info\u001b[39m.\u001b[39mpath)\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    380\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: Path /allah/freqtrade/json_dict/aggTrades/batch_data/batch_0.parquet points to a directory, but only file paths are supported. To construct a nested or union dataset pass a list of dataset objects instead.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/allah/freqtrade/json_dict/binance_API_download.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/binance_API_download.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Combine the batch files into the final Dask DataFrame\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/binance_API_download.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m batch_files \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(batch_output_dir, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbatch_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.parquet\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(csv_files), batch_size)]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/binance_API_download.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m combined_ddf \u001b[39m=\u001b[39m dd\u001b[39m.\u001b[39;49mread_parquet(batch_files)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/binance_API_download.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# Compute the final Dask DataFrame into a Pandas DataFrame\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c696e6f64655f73696e6761706f7265227d/allah/freqtrade/json_dict/binance_API_download.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m combined_df \u001b[39m=\u001b[39m combined_ddf\u001b[39m.\u001b[39mcompute()\n",
      "File \u001b[0;32m/allah/freqtrade/.venv/lib/python3.11/site-packages/dask/backends.py:138\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(e)(\n\u001b[1;32m    139\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling the \u001b[39m\u001b[39m{\u001b[39;00mfuncname(func)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmethod registered to the \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend\u001b[39m}\u001b[39;00m\u001b[39m backend.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOriginal Message: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: Path /allah/freqtrade/json_dict/aggTrades/batch_data/batch_0.parquet points to a directory, but only file paths are supported. To construct a nested or union dataset pass a list of dataset objects instead."
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory where the decompressed CSV files are located\n",
    "csv_extract_dir = '/allah/freqtrade/json_dict/aggTrades/decompressed_csv'\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(csv_extract_dir) if file.endswith('.csv')]\n",
    "\n",
    "# Sort the list of files by their names (assumes filenames are in YYYY-MM-DD.csv format)\n",
    "csv_files.sort()\n",
    "\n",
    "# Set the batch size (number of CSV files to process at a time)\n",
    "batch_size = 10  # You can adjust this based on your available memory\n",
    "\n",
    "# Initialize a Dask DataFrame to store the data\n",
    "combined_ddf = None\n",
    "\n",
    "# Create a progress bar (optional)\n",
    "from tqdm import tqdm\n",
    "progress_bar = tqdm(total=len(csv_files), desc=\"Combining CSV files\")\n",
    "\n",
    "# Directory to store intermediate batch files\n",
    "batch_output_dir = '/allah/freqtrade/json_dict/aggTrades/batch_data'\n",
    "os.makedirs(batch_output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through the CSV files, read each into a Dask DataFrame, and save each batch to a file\n",
    "for i in range(0, len(csv_files), batch_size):\n",
    "    batch_files = csv_files[i:i + batch_size]\n",
    "\n",
    "    # Initialize a batch Dask DataFrame\n",
    "    batch_ddf = None\n",
    "\n",
    "    for csv_file in batch_files:\n",
    "        file_path = os.path.join(csv_extract_dir, csv_file)\n",
    "        df = dd.read_csv(file_path)\n",
    "\n",
    "        if batch_ddf is None:\n",
    "            batch_ddf = df\n",
    "        else:\n",
    "            batch_ddf = dd.concat([batch_ddf, df])\n",
    "\n",
    "        # Update the progress bar (optional)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Save the batch to a file\n",
    "    batch_output_path = os.path.join(batch_output_dir, f'batch_{i}.parquet')\n",
    "    batch_ddf.to_parquet(batch_output_path)\n",
    "\n",
    "    # Close the batch Dask DataFrame\n",
    "    del batch_ddf\n",
    "\n",
    "# Close the progress bar (optional)\n",
    "progress_bar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Parquet files: 100%|██████████| 285/285 [01:20<00:00,  3.55it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directory where the Parquet files are located\n",
    "batch_output_dir = '/allah/freqtrade/json_dict/aggTrades/batch_data'\n",
    "\n",
    "# Initialize an empty list to store the file paths of Parquet files\n",
    "parquet_files_list = []\n",
    "\n",
    "# List all Parquet files in the directory and its subdirectories\n",
    "for root, dirs, files in os.walk(batch_output_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.parquet'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            parquet_files_list.append(file_path)\n",
    "\n",
    "# Now, the 'parquet_files_list' contains the file paths of all the Parquet files in the directory and its subdirectories\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "data_frames = []\n",
    "\n",
    "# Create a progress bar (optional)\n",
    "progress_bar = tqdm(total=len(parquet_files_list), desc=\"Processing Parquet files\")\n",
    "\n",
    "# Loop through the Parquet files in parquet_files_list and apply block1 to each file\n",
    "for parquet_file in parquet_files_list:\n",
    "    part_ddf = dd.read_parquet(parquet_file)\n",
    "    part_df = part_ddf.compute()\n",
    "\n",
    "    # Ensure the 'utc_time' column is in datetime format\n",
    "    part_df['utc_time'] = pd.to_datetime(part_df['transact_time'], unit='ms')\n",
    "\n",
    "    # Set 'utc_time' as the index of the DataFrame\n",
    "    part_df.set_index('utc_time', inplace=True)\n",
    "\n",
    "    # Resample the DataFrame to one row per second\n",
    "    resampled_df = part_df.resample('S').agg({\n",
    "        'agg_trade_id': 'first',\n",
    "        'price': 'ohlc',\n",
    "        'quantity': 'sum',\n",
    "        'first_trade_id': 'first',\n",
    "        'last_trade_id': 'last',\n",
    "        'is_buyer_maker': 'last'\n",
    "    })\n",
    "\n",
    "    # Reset the index to have the 'utc_time' as a column\n",
    "    resampled_df.reset_index(inplace=True)\n",
    "\n",
    "    # Drop the hierarchical column index created by the resample method\n",
    "    resampled_df.columns = resampled_df.columns.droplevel()\n",
    "\n",
    "    # Fill in missing values\n",
    "    resampled_df.fillna(method='ffill', inplace=True)\n",
    "    resampled_df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # Append the result DataFrame to the list\n",
    "    data_frames.append(resampled_df)\n",
    "\n",
    "    # Update the progress bar (optional)\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar (optional)\n",
    "progress_bar.close()\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame in time order\n",
    "combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Now, 'combined_df' contains all the data from the Parquet files in time order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "import zipfile\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, base_url, zip_download_dir, csv_extract_dir, batch_output_dir):\n",
    "        self.base_url = base_url\n",
    "        self.zip_download_dir = zip_download_dir\n",
    "        self.csv_extract_dir = csv_extract_dir\n",
    "        self.batch_output_dir = batch_output_dir\n",
    "\n",
    "    def download_and_extract_data(self, start_date, end_date):\n",
    "        os.makedirs(self.zip_download_dir, exist_ok=True)\n",
    "        os.makedirs(self.csv_extract_dir, exist_ok=True)\n",
    "        download_bar = tqdm(total=(end_date - start_date).days + 1, desc=\"Downloading\")\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            date_str = current_date.strftime('%Y-%m-%d')\n",
    "            link = f'{self.base_url}ETHUSDT-aggTrades-{date_str}.zip'\n",
    "            response = requests.get(link)\n",
    "            if response.status_code == 200:\n",
    "                filename = os.path.join(self.zip_download_dir, f'ETHUSDT-aggTrades-{date_str}.zip')\n",
    "                with open(filename, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                download_bar.update(1)\n",
    "            else:\n",
    "                print(f\"Error: {response.status_code} - Unable to download the ZIP file for {date_str}.\")\n",
    "            current_date += datetime.timedelta(days=1)\n",
    "        download_bar.close()\n",
    "\n",
    "        extract_bar = tqdm(total=len(os.listdir(self.zip_download_dir)), desc=\"Extracting\")\n",
    "        for filename in os.listdir(self.zip_download_dir):\n",
    "            if filename.endswith('.zip'):\n",
    "                with zipfile.ZipFile(os.path.join(self.zip_download_dir, filename), 'r') as zip_ref:\n",
    "                    zip_ref.extractall(self.csv_extract_dir)\n",
    "                extract_bar.update(1)\n",
    "        extract_bar.close()\n",
    "\n",
    "        print(\"All ZIP files downloaded and CSV files extracted successfully.\")\n",
    "\n",
    "    def process_csv_files(self, batch_size):\n",
    "        csv_files = [file for file in os.listdir(self.csv_extract_dir) if file.endswith('.csv')]\n",
    "        csv_files.sort()\n",
    "        combined_ddf = None\n",
    "        progress_bar = tqdm(total=len(csv_files), desc=\"Combining CSV files\")\n",
    "        batch_output_dir = self.batch_output_dir\n",
    "        os.makedirs(batch_output_dir, exist_ok=True)\n",
    "        for i in range(0, len(csv_files), batch_size):\n",
    "            batch_files = csv_files[i:i + batch_size]\n",
    "            batch_ddf = None\n",
    "            for csv_file in batch_files:\n",
    "                file_path = os.path.join(self.csv_extract_dir, csv_file)\n",
    "                df = dd.read_csv(file_path)\n",
    "                if batch_ddf is None:\n",
    "                    batch_ddf = df\n",
    "                else:\n",
    "                    batch_ddf = dd.concat([batch_ddf, df])\n",
    "                progress_bar.update(1)\n",
    "            batch_output_path = os.path.join(batch_output_dir, f'batch_{i}.parquet')\n",
    "            batch_ddf.to_parquet(batch_output_path)\n",
    "            del batch_ddf\n",
    "        progress_bar.close()\n",
    "\n",
    "    def process_parquet_files(self):\n",
    "        parquet_files_list = []\n",
    "        for root, dirs, files in os.walk(self.batch_output_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.parquet'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    parquet_files_list.append(file_path)\n",
    "        data_frames = []\n",
    "        progress_bar = tqdm(total=len(parquet_files_list), desc=\"Processing Parquet files\")\n",
    "        for parquet_file in parquet_files_list:\n",
    "            part_ddf = dd.read_parquet(parquet_file)\n",
    "            part_df = part_ddf.compute()\n",
    "            part_df['utc_time'] = pd.to_datetime(part_df['transact_time'], unit='ms')\n",
    "            part_df.set_index('utc_time', inplace=True)\n",
    "            resampled_df = part_df.resample('S').agg({\n",
    "                'agg_trade_id': 'first',\n",
    "                'price': 'ohlc',\n",
    "                'quantity': 'sum',\n",
    "                'first_trade_id': 'first',\n",
    "                'last_trade_id': 'last',\n",
    "                'is_buyer_maker': 'last'\n",
    "            })\n",
    "            resampled_df.reset_index(inplace=True)\n",
    "            resampled_df.columns = resampled_df.columns.droplevel()\n",
    "            resampled_df.fillna(method='ffill', inplace=True)\n",
    "            resampled_df.fillna(method='bfill', inplace=True)\n",
    "            data_frames.append(resampled_df)\n",
    "            progress_bar.update(1)\n",
    "        progress_bar.close()\n",
    "        combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    def run_data_processing_workflow(self, start_date, end_date, batch_size):\n",
    "        self.download_and_extract_data(start_date, end_date)\n",
    "        self.process_csv_files(batch_size)\n",
    "        self.process_parquet_files()\n",
    "\n",
    "# Example usage:\n",
    "base_url = 'https://data.binance.vision/data/futures/um/daily/aggTrades/ETHUSDT/'\n",
    "zip_download_dir = '/allah/freqtrade/json_dict/binance_aggTrades'\n",
    "csv_extract_dir = '/allah/freqtrade/json_dict/decompressed_csv'\n",
    "batch_output_dir = '/allah/freqtrade/json_dict/aggTrades/batch_data'\n",
    "\n",
    "data_processor = DataProcessor(base_url, zip_download_dir, csv_extract_dir, batch_output_dir)\n",
    "start_date = datetime.date(2023, 1, 1)\n",
    "end_date = datetime.date(2023, 10, 10)\n",
    "batch_size = 10\n",
    "\n",
    "data_processor.run_data_processing_workflow(start_date, end_date, batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
