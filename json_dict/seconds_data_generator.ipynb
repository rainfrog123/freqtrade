{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining CSV files: 100%|██████████| 62/62 [00:01<00:00, 51.16it/s] \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, base_url, zip_download_dir, csv_extract_dir):\n",
    "        self.base_url = base_url\n",
    "        self.zip_download_dir = zip_download_dir\n",
    "        self.csv_extract_dir = csv_extract_dir\n",
    "\n",
    "    def _download_zip_file(self, link, filename):\n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(filename):\n",
    "            print(f\"File {filename} already exists. Skipping download.\")\n",
    "            return True\n",
    "\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - Unable to download the ZIP file for {filename}\")\n",
    "            return False\n",
    "\n",
    "    def _extract_zip_file(self, zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            for file_info in zip_ref.infolist():\n",
    "                # Construct the full path for the extracted file\n",
    "                extracted_file_path = os.path.join(self.csv_extract_dir, file_info.filename)\n",
    "\n",
    "                # Check if the file already exists, and skip extraction if it does\n",
    "                if os.path.exists(extracted_file_path):\n",
    "                    print(f\"File {extracted_file_path} already exists. Skipping extraction.\")\n",
    "                    continue\n",
    "\n",
    "                # Extract the file\n",
    "                zip_ref.extract(file_info, path=self.csv_extract_dir)\n",
    "\n",
    "    def download_and_extract_data(self, start_date, end_date):\n",
    "        os.makedirs(self.zip_download_dir, exist_ok=True)\n",
    "        os.makedirs(self.csv_extract_dir, exist_ok=True)\n",
    "        download_bar = tqdm(total=(end_date - start_date).days + 1, desc=\"Downloading and Extracting\")\n",
    "        \n",
    "        for current_date in (start_date + datetime.timedelta(n) for n in range((end_date - start_date).days + 1)):\n",
    "            date_str = current_date.strftime('%Y-%m-%d')\n",
    "            link = f'{self.base_url}ETHUSDT-aggTrades-{date_str}.zip'\n",
    "            filename = os.path.join(self.zip_download_dir, f'ETHUSDT-aggTrades-{date_str}.zip')\n",
    "\n",
    "            if self._download_zip_file(link, filename):\n",
    "                self._extract_zip_file(filename)\n",
    "            download_bar.update(1)\n",
    "        download_bar.close()\n",
    "\n",
    "    def process_csv_files(self):\n",
    "        csv_files = [file for file in os.listdir(self.csv_extract_dir) if file.endswith('.csv')]\n",
    "        csv_files.sort()\n",
    "        combined_df = None\n",
    "        progress_bar = tqdm(total=len(csv_files), desc=\"Combining CSV files\")\n",
    "        \n",
    "        for csv_file in csv_files:\n",
    "            file_path = os.path.join(self.csv_extract_dir, csv_file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            if combined_df is None:\n",
    "                combined_df = df\n",
    "            else:\n",
    "                combined_df = pd.concat([combined_df, df])\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        progress_bar.close()\n",
    "        return combined_df\n",
    "\n",
    "    def _convert_to_seconds(self, df):\n",
    "        df['utc_time'] = pd.to_datetime(df['transact_time'], unit='ms')\n",
    "        df.set_index('utc_time', inplace=True)\n",
    "        \n",
    "        resampled_df = df.resample('S').agg({\n",
    "            'agg_trade_id': 'first',\n",
    "            'price': 'ohlc',\n",
    "            'quantity': 'sum',\n",
    "            'first_trade_id': 'first',\n",
    "            'last_trade_id': 'last',\n",
    "            'is_buyer_maker': 'last',\n",
    "            # 'transact_time': 'last',\n",
    "        })\n",
    "        resampled_df.reset_index(inplace=True)\n",
    "        resampled_df.columns = resampled_df.columns.droplevel()\n",
    "        resampled_df.fillna(method='ffill', inplace=True)\n",
    "        resampled_df.fillna(method='bfill', inplace=True)\n",
    "        \n",
    "        return resampled_df\n",
    "\n",
    "    def run_data_processing_workflow(self, start_date, end_date):\n",
    "        self.download_and_extract_data(start_date, end_date)\n",
    "        df = self.process_csv_files()\n",
    "        df = self._convert_to_seconds(df)\n",
    "        # define start and end date of df\n",
    "        # df = df[(df['utc_time'] >= start_date) & (df['utc_time'] <= end_date)]\n",
    "        return df\n",
    "\n",
    "    def combine_select_csv_to_df(self, start_date, end_date):\n",
    "        csv_files = [file for file in os.listdir(self.csv_extract_dir) if file.endswith('.csv')]\n",
    "        csv_files.sort()\n",
    "        combined_df = None\n",
    "        progress_bar = tqdm(total=len(csv_files), desc=\"Combining CSV files\")\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            # Extract the date part from the file name and convert it to a date\n",
    "            date_str = '-'.join(csv_file.split('-')[-3:]).replace('.csv', '')\n",
    "            file_date = datetime.datetime.strptime(date_str, '%Y-%m-%d').date()\n",
    "\n",
    "            # Check if the file date is within the specified date range\n",
    "            if start_date <= file_date <= end_date:\n",
    "                file_path = os.path.join(self.csv_extract_dir, csv_file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                df = self._convert_to_seconds(df)  # Apply _convert_to_seconds\n",
    "                combined_df = pd.concat([combined_df, df]) if combined_df is not None else df\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "        return combined_df\n",
    "\n",
    "\n",
    "\n",
    "    def delete_all_data(self):\n",
    "        os.system(f'rm -rf {self.zip_download_dir}')\n",
    "        os.system(f'rm -rf {self.csv_extract_dir}')\n",
    "\n",
    "# Example usage:\n",
    "base_url = 'https://data.binance.vision/data/futures/um/daily/aggTrades/ETHUSDT/'\n",
    "zip_download_dir = '/allah/freqtrade/json_dict/binance_aggTrades'\n",
    "csv_extract_dir = '/allah/freqtrade/json_dict/decompressed_csv'\n",
    "\n",
    "data_processor = DataProcessor(base_url, zip_download_dir, csv_extract_dir)\n",
    "start_date = datetime.date(2023, 9, 15)\n",
    "end_date = datetime.date(2023, 10, 1)\n",
    "\n",
    "# df = data_processor.run_data_processing_workflow(start_date, end_date)\n",
    "# df = data_processor.combine_select_csv_to_df(datetime.date(2022, 9, 13), datetime.date(2023, 1, 1))\n",
    "df = data_processor.run_data_processing_workflow(datetime.date(2022, 9, 13), datetime.date(2023, 1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            agg_trade_id     open     high      low    close  \\\n",
      "0      2023-09-13 00:00:04  1.378371e+09  1592.29  1592.29  1592.01  1592.01   \n",
      "1      2023-09-13 00:00:05  1.378371e+09  1592.02  1592.02  1592.01  1592.01   \n",
      "2      2023-09-13 00:00:06  1.378371e+09  1592.02  1592.33  1592.01  1592.33   \n",
      "3      2023-09-13 00:00:07  1.378371e+09  1592.24  1592.25  1592.13  1592.14   \n",
      "4      2023-09-13 00:00:08  1.378371e+09  1592.12  1592.13  1592.08  1592.08   \n",
      "...                    ...           ...      ...      ...      ...      ...   \n",
      "345583 2023-09-16 23:59:55  1.380247e+09  1633.68  1633.68  1633.68  1633.68   \n",
      "345584 2023-09-16 23:59:56  1.380247e+09  1633.68  1633.68  1633.68  1633.68   \n",
      "345585 2023-09-16 23:59:57  1.380247e+09  1633.68  1633.68  1633.68  1633.68   \n",
      "345586 2023-09-16 23:59:58  1.380247e+09  1633.68  1633.68  1633.68  1633.68   \n",
      "345587 2023-09-16 23:59:59  1.380247e+09  1633.68  1633.68  1633.68  1633.68   \n",
      "\n",
      "        quantity  first_trade_id  last_trade_id  is_buyer_maker  \\\n",
      "0        168.285    3.226147e+09   3.226147e+09             1.0   \n",
      "1          9.655    3.226147e+09   3.226147e+09             1.0   \n",
      "2        373.095    3.226147e+09   3.226148e+09             0.0   \n",
      "3         75.977    3.226148e+09   3.226148e+09             0.0   \n",
      "4          3.012    3.226148e+09   3.226148e+09             1.0   \n",
      "...          ...             ...            ...             ...   \n",
      "345583     0.199    3.232166e+09   3.232166e+09             0.0   \n",
      "345584     0.000    3.232166e+09   3.232166e+09             0.0   \n",
      "345585     0.057    3.232166e+09   3.232166e+09             0.0   \n",
      "345586     0.000    3.232166e+09   3.232166e+09             0.0   \n",
      "345587     0.015    3.232166e+09   3.232166e+09             0.0   \n",
      "\n",
      "                 timestamp  \n",
      "0      2023-09-16 23:59:59  \n",
      "1      2023-09-16 23:58:59  \n",
      "2      2023-09-16 23:57:59  \n",
      "3      2023-09-16 23:56:59  \n",
      "4      2023-09-16 23:55:59  \n",
      "...                    ...  \n",
      "345583 2023-01-20 00:16:59  \n",
      "345584 2023-01-20 00:15:59  \n",
      "345585 2023-01-20 00:14:59  \n",
      "345586 2023-01-20 00:13:59  \n",
      "345587 2023-01-20 00:12:59  \n",
      "\n",
      "[345588 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "current_timestamp = pd.Timestamp('2023-09-16 23:59:59')\n",
    "\n",
    "# Iterate through the DataFrame in reverse order and modify timestamps\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'timestamp'] = current_timestamp\n",
    "    current_timestamp -= pd.Timedelta(minutes=1)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
